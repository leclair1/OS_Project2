#define _GNU_SOURCE
#include "serial.h"

#include <pthread.h>
#include <dirent.h>
#include <errno.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <sys/stat.h>
#include <zlib.h>

/* --------------------------------------------------------------------------
   here is an outline of the plan
   1. we scan input_dir for .txt files and sort names in lex order
   2. we build a results table with one slot per file
   3. we push one job per file into a shared queue
   4. workers read and compress in parallel
   5. we write members in lex order using the starter writer
   note we keep final writes single threaded to avoid container issues
   -------------------------------------------------------------------------- */

/* ----------------------------- data models ------------------------------ */

typedef struct job {
    char *path;           // full path to disk file
    char *logical_name;   // name to store inside the zip
    int index;            // position in lex order
    struct job *next;     // next job in queue
} job_t;                    // job structure

typedef struct {
    unsigned char **out_bufs;  // one buffer per file
    size_t *out_sizes;         // compressed size per file
    int count;                // number of files
} results_t;                 // results table

/* ---------------------------- work queue core --------------------------- */

typedef struct {
    pthread_mutex_t mtx;        // mutex for queue operations
    pthread_cond_t cv_have;     // condition var for job availability
    pthread_cond_t cv_done; // condition var for queue completion
    job_t *head;             // head of the job queue
    job_t *tail;                // tail of the job queue
    int open;                // queue is open while producer pushes
    int active;                 // workers currently holding a job
    results_t *res;              // pointer to results table
} queue_t;                       // queue structure

// here we set up locks and flags
static void queue_init(queue_t *q, results_t *res) {
    pthread_mutex_init(&q->mtx, NULL);      // initialize mutex
    pthread_cond_init(&q->cv_have, NULL);   // initialize condition variable
    pthread_cond_init(&q->cv_done, NULL); // initialize condition variable
    q->head = q->tail = NULL;       // initialize head and tail to null
    q->head = q->tail = NULL;       // initialize head and tail to null
    q->open = 1;        // initialize open to 1
    q->active = 0;      // initialize active to 0
    q->res = res;    // store results table pointer
    q->res = res; // store results table pointer
}

// here we close the queue so workers stop waiting after drain
static void queue_close(queue_t *q) { 
    pthread_mutex_lock(&q->mtx); // lock mutex
    q->open = 0; // set open to 0 
    pthread_cond_broadcast(&q->cv_have); // broadcast condition variable
    pthread_mutex_unlock(&q->mtx); // unlock mutex
}

// now we push a job by appending and then signal the condition var
static void queue_push(queue_t *q, job_t *j) {
    j->next = NULL; // set next to null
    pthread_mutex_lock(&q->mtx); // lock mutex
    append_to_tail(&q->tail, j); // append to tail
    pthread_cond_signal(&q->cv_have); // signal condition variable
    pthread_mutex_unlock(&q->mtx); // unlock mutex
 
// this section a consumer pops a job or returns null if closed and empty
static job_t* queue_pop(queue_t *q) {
    pthread_mutex_lock(&q->mtx); // lock mutex
    while (q->head == NULL && q->open == 1) { // while no head and open
        pthread_cond_wait(&q->cv_have, &q->mtx); // waiting for condition variable
    }
    job_t *job = q->head; // head
    if (job) { // if job is not null
        q->head = job->next; // set head to next
        if (q->head == NULL) { // if head is null
            q->tail = NULL; // set tail to null
        }
        job->next = NULL; // set next to null
        q->active++; // increment active
    }
    pthread_mutex_unlock(&q->mtx); // unlock mutex
    return job; // return job or null
}

// here a consumer reports done so we can detect global completion
static void queue_task_done(queue_t *q) {
    pthread_mutex_lock(&q->mtx); // lock mutex
    q->active--; // decrement active
    // if closed and empty and active is zero then we signal cv_done
    if (q->open == 0 && q->head == NULL && q->active == 0) { // if open is zero and head is null and active is zero
        pthread_cond_signal(&q->cv_done); // signal condition variable
    }
    pthread_mutex_unlock(&q->mtx); // unlock mutex
}

// here the producer waits until queue is empty and no consumer is active
static void queue_wait_all(queue_t *q) {
    pthread_mutex_lock(&q->mtx); // lock mutex
    // while open or head not null or active not zero wait on cv_done
    while (q->open == 1 || q->head != NULL || q->active != 0) {
        pthread_cond_wait(&q->cv_done, &q->mtx); // waiting for condition variable
    }
    pthread_mutex_unlock(&q->mtx); // unlock mutex
}

/* ------------------------------ io helpers ------------------------------ */

// here we join dir and base name
static char* join_path(const char *dir, const char *base) {
    size_t dir_len = strlen(dir); // get length of dir
    size_t base_len = strlen(base); // get length of base
    size_t len = dir_len + 1 + base_len + 1; // now we calculate length of new path
    char *path = (char*)malloc(len); // and allocate memory for new path
    if (!path) { // but if allocation failed
        return NULL; // return null
    }
    strcpy(path, dir); // here we copy dir to path
    // this section we allocate buffer for dir plus optional slash plus base plus null
    if (dir[dir_len - 1] != '/') { // if dir does not end with slash
        path[dir_len] = '/'; // add slash
        dir_len++; // and increment dir_len
    }
    strcpy(path + dir_len, base); // copy base to path
    return path; // return new path
}

// here we read a whole file into memory
static int read_whole_file(const char *path, unsigned char **buf, size_t *len) {
    FILE *file = fopen(path, "rb"); // open file in read binary mode
    if (!file) { // if file is null
        return -1; // return -1
    }
    fseek(file, 0, SEEK_END); // seek to end of file
    long size = ftell(file); // get size of file
    rewind(file); // rewind file
    *buf = (unsigned char*)malloc(size); // allocate memory for buffer
    if (!*buf) { // if buffer is null

    // we need to malloc size bytes and free
    if (fread(*buf, 1, size, file) != size) { // so if read failed
        fclose(file); // close file
        free(*buf); // free buffer
        return -1; // and return -1
    }
    // finally t the end we set length and return 0
    fclose(file); // close file
    *len = size; // set length
    return 0; // return 0
}

/* ---------------------------- compression stub -------------------------- */

// here we begin with a simple pass through so we can bring up threads first
// later we will replace this with real zlib deflate
static int deflate_buffer(const unsigned char *in, size_t in_len,
                          unsigned char **out, size_t *out_len) {
    // todo for now allocate out buffer of in_len and memcpy
    // todo set *out_len and return 0 on success
    // todo return nonzero on failure
    return -1;
}

/* ------------------------------ worker code ----------------------------- */

typedef struct {
    // todo stash whatever state each worker needs (likely queue pointer)
    queue_t *q;
} worker_arg_t;

// here we free a job holder
static void free_job(job_t *j) {
    // todo free path, logical_name, and then the job struct
}

// here each worker loops until the queue is closed and empty
static void* worker_main(void *arg) {
    // todo cast arg to worker_arg_t and grab queue pointer
    // todo loop popping jobs until queue_pop returns null
    // todo read each file, handle failures, and record results
    // todo compress raw bytes, update q->res, and clean up buffers
    // todo free the job, call queue_task_done, and continue
    return NULL;
}

/* ----------------------- directory scan and ordering -------------------- */

static int ends_with_txt(const char *s) {
    // todo check if s ends with .txt
    return 0;
}

static int cmp_lex(const void *a, const void *b) {
    const char * const *pa = (const char * const *)a;
    const char * const *pb = (const char * const *)b;
    return strcmp(*pa, *pb);
}

// here we list .txt files in lex order and return a heap array
static char** list_txt_lex(const char *dir, int *out_count) {
    // todo open directory
    // todo collect names that end with .txt into a heap array
    // todo qsort with cmp_lex
    // todo set out_count and return names or null on error
    return NULL;
}

/* --------------------------- zip writer glue ---------------------------- */

// here we plug into the starter zip writer
static int zip_begin(const char *output_zip) {
    // todo call starter open function
    (void)output_zip;
    return 0;
}

static int zip_write_member(const char *logical_name,
                            const unsigned char *bytes, size_t nbytes) {
    // todo call starter helper to write one member
    (void)logical_name; (void)bytes; (void)nbytes;
    return 0;
}

static int zip_end(void) {
    // todo call starter close function
    return 0;
}

/* ----------------------------- public entry ----------------------------- */

static int clamp_threads(int requested) {
    if (requested < 1) requested = 1;
    if (requested > 19) requested = 19;
    return requested;
}

int run_parallel(const char *input_dir, const char *output_zip, int requested_threads) {
    // here we list names in lex order
   int count = 0;
    char **lex = list_txt_lex(input_dir, &count);
    if (!lex || count == 0) {
 // todo free lex if partially allocated
        if (lex) free(lex);
        return 0;
    }

    // here we make the results table
    results_t res = (results_t){0};
    res.count = count;
    res.out_bufs = (unsigned char**)calloc((size_t)count, sizeof(unsigned char*));
    res.out_sizes = (size_t*)calloc((size_t)count, sizeof(size_t));
    if (!res.out_bufs || !res.out_sizes) {
        // todo free res arrays and lex
        free(res.out_bufs);
        free(res.out_sizes);
        for (int i = 0; i < count; i++) free(lex[i]);
        free(lex);
        return -1;
        
    }

    // here we init the queue
    queue_t q;
    queue_init(&q, &res);

    // here we start worker threads
    int nthreads = clamp_threads(requested_threads);
    pthread_t *ths = (pthread_t*)malloc((size_t)nthreads * sizeof(pthread_t));
    worker_arg_t wa = { .q = &q };
    for (int i = 0; i < nthreads; ++i) {
        // todo create thread that runs worker_main with wa
      pthread_create(&ths[i], NULL, worker_main, &wa);
    }

    // here we enqueue jobs
    for (int i = 0; i < count; ++i) {
        job_t *j = (job_t*)calloc(1, sizeof(job_t));
        // todo set j->logical_name to strdup of lex[i]
        // todo set j->path with join_path
        // todo set j->index
        // todo push into queue
    }

    // here we close the queue and wait for all work to finish
    // todo queue_close
    // todo queue_wait_all

    // here we join all workers
    for (int i = 0; i < nthreads; ++i) {
        // todo pthread_join
    }
    free(ths);

    // here we write members in lex order
    if (zip_begin(output_zip) != 0) {
        // todo decide error policy
    }
    for (int i = 0; i < count; ++i) {
        if (res.out_bufs[i] && res.out_sizes[i] > 0) {
            zip_write_member(lex[i], res.out_bufs[i], res.out_sizes[i]);
        } else {
            // todo decide how we handle failures
        }
    }
    zip_end();

    // here we clean up
    for (int i = 0; i < count; ++i) {
        // todo free lex[i]
        // todo free res.out_bufs[i]
    }
    // todo free lex
    // todo free res.out_bufs and res.out_sizes

    return 0;
}
